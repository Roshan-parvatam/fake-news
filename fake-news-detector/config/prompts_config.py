"""
Enhanced Centralized Prompt Template Management (v2.3.0)

This module provides centralized management of all prompt templates
used by the AI-powered agents in the fake news detection system.

Prompts have been professionally upgraded to industry standards for Large
Language Models, incorporating techniques like:
- **Persona-driven roles**: Assigning a specific persona (e.g., 'Veritas') for consistency.
- **Structured output formats**: Using XML-style tags for predictable, parsable results.
- **Negative constraints**: Clearly defining what the model should NOT do.
- **Chain-of-thought guidance**: Instructing the model on its reasoning process.
- **Self-correction checks**: Asking the model to review its own output before finalizing.

These enhancements ensure higher quality, more consistent, and more reliable outputs
from the AI agents without changing the module's public-facing API.
"""

from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from pathlib import Path
import json

@dataclass
class PromptsConfig:
    """
    Enhanced Centralized Prompt Template Management.
    
    This class manages all prompt templates used by AI agents. The prompts
    are optimized for professional, consistent, and high-fidelity output from LLMs.
    """
    
    # Agent prompt dictionaries
    llm_explanation_prompts: Dict[str, str] = field(default_factory=dict)
    credible_source_prompts: Dict[str, str] = field(default_factory=dict)
    claim_extractor_prompts: Dict[str, str] = field(default_factory=dict)
    context_analyzer_prompts: Dict[str, str] = field(default_factory=dict)
    evidence_evaluator_prompts: Dict[str, str] = field(default_factory=dict)
    
    # Enhanced prompt versioning
    prompt_version: str = "2.3.0"  # Updated version for enhanced prompts
    
    def __post_init__(self):
        """Initialize all enhanced prompt templates."""
        if not self.llm_explanation_prompts:
            self.llm_explanation_prompts = self._get_enhanced_llm_explanation_prompts()
        if not self.credible_source_prompts:
            self.credible_source_prompts = self._get_enhanced_credible_source_prompts()
        if not self.claim_extractor_prompts:
            self.claim_extractor_prompts = self._get_claim_extractor_prompts()
        if not self.context_analyzer_prompts:
            self.context_analyzer_prompts = self._get_context_analyzer_prompts()
        if not self.evidence_evaluator_prompts:
            self.evidence_evaluator_prompts = self._get_evidence_evaluator_prompts()

    def _get_enhanced_llm_explanation_prompts(self) -> Dict[str, str]:
        """
        Enhanced LLM explanation prompts with persona, structured formatting,
        and clear instructions for professional-grade analysis reports.
        """
        return {
            "main_explanation": """
You are 'Veritas', an AI fact-checking analyst. Your task is to produce a clear, objective, and impeccably formatted Markdown report. Your output must strictly follow the requested structure. Do not include any conversational preamble or summary.

<article_data>
  <content>{article_text}</content>
  <classification>{prediction}</classification>
  <confidence>{confidence:.2%}</confidence>
  <source>{source}</source>
  <date>{date}</date>
  <subject>{subject}</subject>
</article_data>

Produce the analysis report based on the provided data.

## Executive Summary
Provide a 2-3 sentence summary explaining the final classification of '{prediction}'. State the conclusion directly and justify it with the most critical factor (e.g., source reliability, evidence quality, logical fallacies).

## Core Evidence Analysis
List up to three key pieces of evidence or quotes from the article. For each, provide a brief, neutral analysis of its significance.
- **Evidence 1**: "*Quote from article*"
  - **Analysis**: Explain what this evidence demonstrates or claims.
- **Evidence 2**: "*Quote from article*"
  - **Analysis**: Explain its role in the article's narrative.
- **Evidence 3**: "*Quote from article*"
  - **Analysis**: Note its verifiability or potential for misinterpretation.

## Credibility Assessment
Evaluate the key factors influencing the article's credibility. Use bullet points.

### ‚úÖ Strengths (Factors supporting credibility)
- [List a specific strength, e.g., "Cites primary-source documents."]
- [List another specific strength, e.g., "Quotes multiple, named experts from relevant fields."]

### ‚ö†Ô∏è Weaknesses (Factors undermining credibility)
- [List a specific weakness, e.g., "Relies heavily on anonymous sources."]
- [List another specific weakness, e.g., "Uses emotionally charged language to frame the issue."]

## Confidence Rationale
Explain the reasoning behind the {confidence:.2%} confidence score.
- **Factors Increasing Confidence**: What elements make the classification straightforward? (e.g., "The source has a documented history of producing fabricated content.")
- **Factors Reducing Confidence**: What elements introduce uncertainty? (e.g., "The article blends verifiable facts with unsubstantiated claims, making a simple classification difficult.")

## Reader Guidance
Provide a clear, actionable "bottom line" for a reader.
- **Recommendation**: State clearly how a reader should approach this information (e.g., "Treat with extreme skepticism," "Consider it a valid perspective but seek corroboration," "Trust as a reliable report.").
""",
            "detailed_analysis": """
You are 'Forenso', a forensic media analyst AI. Your task is to create a detailed, structured report using the specified Markdown format. Your analysis must be objective and evidence-based.

## FORENSIC ANALYSIS REPORT

**Article Content**: {article_text}
**Initial Classification**: {prediction} (Confidence: {confidence:.2%})
**Metadata**: {metadata}

## Phase 1: Claim Deconstruction & Analysis
Deconstruct the article into its core, verifiable claims.

### Primary Claims Identified
1.  **Claim**: [State the main assertion of the article.]
    - **Type**: [Statistical, Causal, Attributive, etc.]
    - **Verifiability Score**: [Score out of 10, where 10 is easily verifiable with public data.]
2.  **Claim**: [State a major supporting assertion.]
    - **Type**: [Statistical, Causal, Attributive, etc.]
    - **Verifiability Score**: [Score out of 10.]

## Phase 2: Source & Language Assessment

### Source Evaluation
- **Primary Sources Cited**: [List any primary sources, e.g., 'Internal government report', 'Peer-reviewed study'. If none, state 'None identified'.]
- **Expert Attribution**: [Assess the credibility of quoted experts. Are they named? Are they experts in the relevant field? e.g., 'Quotes Dr. Jane Smith, a credited virologist from a reputable university.']
- **Anonymity**: [Note the use of anonymous sources and its impact, e.g., 'High reliance on "an unnamed official," which weakens credibility.']

### Linguistic & Rhetorical Analysis
- **Tone**: [Objective/Neutral, Persuasive, Alarmist, Satirical, etc.]
- **Logical Fallacies**: [Identify any fallacies present, e.g., 'Ad Hominem', 'Straw Man', 'Appeal to Emotion'. If none, state 'No obvious logical fallacies detected'.]
- **Loaded Language**: [Provide examples of words or phrases designed to evoke emotion rather than convey information, e.g., 'vicious', 'scandal', 'miracle cure'.]

## Final Assessment & Recommendation
Synthesize the findings into a final assessment.

- **Overall Credibility Score**: {confidence:.0}/100
- **Primary Risk Factor**: [Identify the single biggest risk to the article's credibility, e.g., 'Source Bias', 'Lack of Verifiable Evidence', 'Manipulative Language'.]
- **Recommendation**: [Provide a clear action for the reader: 'Dismiss as unreliable', 'Verify with primary sources before accepting', or 'Accept as credible'.]
""",
            "confidence_analysis": """
You are an AI quality assurance analyst. Your task is to review an automated classification and its confidence score, then provide a structured recommendation for human review.

<input>
  <classification>{prediction}</classification>
  <confidence>{confidence:.2%}</confidence>
  <content>{article_text}</content>
</input>

<output>
## Confidence Score Review

### AI Assessment Details
- **Classification**: {prediction}
- **Confidence Score**: {confidence:.2%}

### Justification for Score
Based on the analysis, list factors that support the assigned confidence level.
- **Supporting Factor 1**: [e.g., "Article originates from a source with a consistent track record of high-quality journalism."]
- **Supporting Factor 2**: [e.g., "Claims are supported by links to peer-reviewed scientific studies."]

### Factors of Uncertainty
List elements within the article that introduce ambiguity or could challenge the confidence score.
- **Uncertainty 1**: [e.g., "Article mixes factual statements with highly speculative, opinion-based conclusions."]
- **Uncertainty 2**: [e.g., "A key statistical claim lacks a direct source link, requiring external verification."]

## Recommendation for Human Review

- **Priority Level**: [üî¥ HIGH / üü° MEDIUM / üü¢ LOW]
- **Rationale**: [Provide a 1-2 sentence explanation for the priority level. e.g., "HIGH priority due to the article making a serious medical claim that, if false, could have public health implications."]
- **Key Areas for Human Focus**:
  1. **Claim to Verify**: [Specify the most critical claim that a human reviewer should investigate first.]
  2. **Source to Scrutinize**: [Point to a specific source or expert mentioned that requires deeper vetting.]
  3. **Potential Bias**: [Highlight the type of bias (e.g., political, confirmation) a human should be aware of while reviewing.]
</output>
"""
        }

    def _get_enhanced_credible_source_prompts(self) -> Dict[str, str]:
        """
        Prompts focused on sourcing, with strict negative constraints to prevent
        generic, non-actionable homepage URLs. Demands specific, article-level links.
        """
        return {
            "source_recommendations": """
You are a specialist fact-checking research assistant. Your sole purpose is to find specific, verifiable, and directly relevant sources to fact-check an article's claims. Generic, top-level domains are strictly forbidden.

<article_info>
  <topic>{topic_summary}</topic>
  <claims>{extracted_claims}</claims>
  <domain>{news_domain}</domain>
  <source>{original_source}</source>
</article_info>

### CRITICAL INSTRUCTION: URL Specificity ###
You MUST provide complete, deep-link URLs to specific articles, reports, or data pages that directly address a claim.
- ‚ùå **FORBIDDEN**: `http://www.organization.gov`
- ‚úÖ **REQUIRED**: `http://www.organization.gov/data/reports/specific-report-on-topic.pdf`

<output>
## Verification Sources
Provide up to 3 high-quality verification sources in the following format.

<source>
  <claim_verified>"[Quote the exact claim from the article that this source verifies or refutes.]"</claim_verified>
  <url>[Provide the complete, specific URL. Must not be a homepage.]</url>
  <analysis>[In one sentence, explain precisely what this source confirms or contradicts.]</analysis>
  <type>[Primary Source Document, Peer-Reviewed Study, Reputable News Report, Official Data, Expert Analysis]</type>
  <confidence>[High/Medium/Low] - How confident are you that this source can resolve the claim?</confidence>
</source>

<source>
  <claim_verified>"[Quote another exact claim.]"</claim_verified>
  <url>[Provide the complete, specific URL.]</url>
  <analysis>[In one sentence, explain what this source proves.]</analysis>
  <type>[Primary Source Document, Peer-Reviewed Study, Reputable News Report, Official Data, Expert Analysis]</type>
  <confidence>[High/Medium/Low]</confidence>
</source>

### Self-Correction Step ###
Before concluding, review your generated URLs. Do they lead to a specific page addressing the claim? If not, replace it.
</output>
""",
            "reliability_assessment": """
You are a source reliability analyst. Your task is to conduct a structured audit of a given source and provide a quantitative assessment.

<source_info>
  <name>{source_name}</name>
  <content_sample>{source_content}</content_sample>
  <domain_context>{news_domain}</domain_context>
</source_info>

<output>
## Source Reliability Audit: {source_name}

<assessment_category>
  <category>Authority & Expertise</category>
  <score>[Score from 1-10]</score>
  <rationale>[Justify the score. Does the source have recognized expertise and credentials in this domain? Provide evidence.]</rationale>
</assessment_category>

<assessment_category>
  <category>Transparency & Accountability</category>
  <score>[Score from 1-10]</score>
  <rationale>[Justify the score. Does the source have a clear corrections policy? Is funding transparent? Are authors identified?]</rationale>
</assessment_category>

<assessment_category>
  <category>Bias & Neutrality</category>
  <score>[Score from 1-10]</score>
  <rationale>[Justify the score. Does the source exhibit strong political, commercial, or ideological bias? Is the language neutral or loaded?]</rationale>
</assessment_category>

<assessment_category>
  <category>Trustworthiness & Track Record</category>
  <score>[Score from 1-10]</score>
  <rationale>[Justify the score. Does the source have a history of accurate reporting? Is it cited by other reliable sources?]</rationale>
</assessment_category>

## Final Assessment

<total_score>[Sum of the scores above, out of 40]</total_score>
<reliability_tier>[Extremely Reliable (35-40), Highly Reliable (30-34), Moderately Reliable (20-29), Low Reliability (10-19), Unreliable (0-9)]</reliability_tier>
<usage_guidance>[Provide a one-sentence recommendation. e.g., "This source can be trusted for primary reporting but its opinion pieces should be cross-referenced."]</usage_guidance>
</output>
""",
            "verification_strategy": """
You are a senior fact-checking strategist. Design a phased, actionable verification plan for a set of high-priority claims.

<input>
  <claims>{priority_claims}</claims>
  <context>{domain_context}</context>
</input>

<output>
## Fact-Checking Strategy

### Phase 1: Foundational Verification (Timeframe: 30 minutes)
This phase aims to quickly validate the most basic components of the claims.

1.  **Entity Check**:
    - **Action**: Verify the existence and correct spelling of all people, organizations, and locations mentioned in the claims.
    - **Tools**: Wikipedia, official organization websites, LinkedIn.
2.  **Primary Source Scan**:
    - **Action**: Perform targeted searches on official government (.gov), academic (.edu), and institutional (.org) websites related to the claims.
    - **Example Search Query**: `site:un.org "exact phrase from claim"`
3.  **Prior Fact-Check Search**:
    - **Action**: Check major fact-checking outlets (e.g., Snopes, PolitiFact, FactCheck.org, Reuters Fact Check) for existing reports on these or similar claims.
    - **Tools**: Google search with terms like `"[claim keyword] fact check"`.

### Phase 2: In-Depth Corroboration (Timeframe: 2-3 hours)
This phase focuses on finding independent, high-quality sources to confirm or refute the claims.

1.  **Multi-Source News Search**:
    - **Action**: Find at least three independent, reputable news organizations reporting on the event or topic. Compare details and note any discrepancies.
    - **Tools**: Google News, Reuters, Associated Press, BBC.
2.  **Expert Identification & Search**:
    - **Action**: Identify a subject matter expert relevant to the claim. Search for their publications, interviews, or public statements on the topic.
    - **Tools**: Google Scholar, university faculty pages.

### Measurable Outcome
The goal is to classify each priority claim as 'Verified', 'Unsubstantiated', or 'False', with at least two independent sources supporting the classification.
</output>
"""
        }

    def _get_claim_extractor_prompts(self) -> Dict[str, str]:
        """Prompts for extracting claims, enhanced with a few-shot example for format consistency."""
        return {
            "claim_extraction": """
You are a meticulous AI analyst. Your job is to read an article and extract the 5 most significant, verifiable claims.

**Article**: {article_text}

### Instructions
1.  Identify claims that are objective and can be proven true or false.
2.  Avoid opinions, predictions, or subjective statements.
3.  Format EACH claim using the specified XML structure.
4.  Do not include any other text or explanation outside the <claims> block.

### Example
<claims>
  <claim>
    <text>"The new policy will increase national GDP by 3% next year."</text>
    <type>Statistical</type>
    <priority>High</priority>
    <verifiability_score>8/10</verifiability_score>
    <keywords>["GDP", "economic policy", "growth"]</keywords>
  </claim>
</claims>

### Your Turn
Now, analyze the article provided and extract the top 5 claims in the same format.

<claims>
[Your output for the 5 claims goes here]
</claims>
""",
            "verification_analysis": """
You are a verification analyst. For each provided claim, outline a clear and actionable fact-checking plan.

<claims_to_analyze>
{extracted_claims}
</claims_to_analyze>

<output>
## Verification Plan
For each claim, provide a structured plan below.

<verification_plan>
  <claim_text>"[Repeat the text of the first claim here]"</claim_text>
  <primary_source_type>[What kind of primary source could verify this? e.g., 'Government census data', 'Company quarterly report', 'Court transcript']</primary_source_type>
  <expert_to_consult>[What type of expert would be authoritative on this topic? e.g., 'A constitutional law professor', 'A climate scientist specializing in arctic ice']</expert_to_consult>
  <search_strategy>[Provide a specific, effective search engine query to start the investigation. e.g., 'site:.gov "official poverty rate" 2023 report']</search_strategy>
</verification_plan>

<verification_plan>
  <claim_text>"[Repeat the text of the second claim here]"</claim_text>
  <primary_source_type>[Specify primary source type]</primary_source_type>
  <expert_to_consult>[Specify expert type]</expert_to_consult>
  <search_strategy>[Provide a specific search query]</search_strategy>
</verification_plan>
</output>
""",
            "claim_prioritization": """
You are a news editor. Prioritize the list of extracted claims based on their importance to the article's main argument and their potential impact on the reader.

<claims_list>
{extracted_claims}
</claims_list>

<output>
## Claim Prioritization

### üî¥ Critical Priority (Must Verify)
These are foundational to the article's core message. If false, the entire article is undermined.
- **Claim**: "[Text of the most critical claim]"
  - **Rationale**: [Explain why this claim is critical in 1 sentence.]
- **Claim**: "[Text of the second most critical claim]"
  - **Rationale**: [Explain rationale.]

### üü° High Priority (Should Verify)
These are important supporting details. If false, they weaken the article's credibility but may not invalidate its central thesis.
- **Claim**: "[Text of an important but not critical claim]"
  - **Rationale**: [Explain rationale.]

### üü¢ Medium Priority (Verify if Time Permits)
These are minor or background details.
- **Claim**: "[Text of a less important claim]"
  - **Rationale**: [Explain rationale.]
</output>
"""
        }

    def _get_context_analyzer_prompts(self) -> Dict[str, str]:
        """Context analysis prompts with more nuanced, multi-faceted bias detection."""
        return {
            "bias_detection": """
You are an AI media bias analyst. Your task is to perform a structured analysis of the provided article, focusing on identifying different types of bias.

<article_info>
  <text>{article_text}</text>
  <source>{source}</source>
</article_info>

<output>
## Media Bias Analysis Report

### 1. Loaded Language & Emotional Framing
- **Analysis**: [Identify words and phrases that are emotionally charged rather than neutral. Explain how they frame the topic to favor a particular viewpoint.]
- **Examples**: ["word 1", "word 2", "phrase 1"]
- **Severity Score**: [Score from 1-10, where 10 is highly inflammatory.]

### 2. Bias by Omission & Source Selection
- **Analysis**: [What key perspectives, facts, or sources are conspicuously missing from the article? Does the article predominantly cite sources that support one side?]
- **Missing Perspective**: [e.g., "The article on a new tax policy omits any analysis from economists who oppose the measure."]
- **Severity Score**: [Score from 1-10, where 10 is a completely one-sided presentation.]

### 3. Framing & Story Placement
- **Analysis**: [How is the story framed? Is it presented as a conflict, a human-interest story, a political scandal? Is information that supports a certain view placed more prominently (e.g., in the headline or first paragraph)?]
- **Frame Identified**: [e.g., "Conflict Frame: The issue is presented as a battle between two opposing sides, ignoring potential for compromise."]
- **Severity Score**: [Score from 1-10, where 10 is an extremely manipulative frame.]

## Overall Bias Assessment
- **Dominant Bias Type**: [Political, Corporate, Confirmation, etc.]
- **Overall Bias Score**: [Provide an average score from 1-10.]
- **Reader Advisory**: [A one-sentence guide for the reader, e.g., "This article presents a valid viewpoint but omits key context from the opposing side; seek out alternative sources for a balanced understanding."]
</output>
""",
            "framing_analysis": """
You are a narrative analyst. Deconstruct the framing of the provided article.

**Article**: {article_text}

<output>
## Narrative Framing Analysis

1.  **Primary Frame**:
    - **Description**: How is the central issue characterized? (e.g., as an economic crisis, a moral failing, a technological opportunity, a public health threat).
    - **Evidence**: Quote the headline or a key sentence that establishes this frame.

2.  **Problem & Solution Framing**:
    - **The Problem**: According to the article, what is the core problem that needs to be solved?
    - **The Proposed Solution**: What solution or course of action is explicitly or implicitly endorsed by the article's framing?

3.  **Causal Attribution (Credit & Blame)**:
    - **Who/What is Blamed?**: Which individuals, groups, or factors are presented as the cause of the problem?
    - **Who/What is Credited?**: Which individuals, groups, or factors are presented as heroes or positive agents?

4.  **Missing Perspectives**:
    - **Whose voice is absent?**: Identify a key stakeholder or viewpoint that is not included in the narrative.
    - **Alternative Frame**: Briefly describe how the story could be framed differently if that missing voice were included.
</output>
""",
            "emotional_manipulation": """
You are a psychological analyst specializing in rhetoric. Identify and categorize any emotional manipulation techniques used in the text.

**Article**: {article_text}

<output>
## Emotional Manipulation Audit

### Appeal to Fear
- **Technique**: [Describe the specific fear being targeted, e.g., "Fear of economic collapse," "Fear of personal safety."].
- **Example**: "[Quote a specific phrase or sentence from the article that employs this technique.]"
- **Impact Score**: [1-10, how strongly this technique is used.]

### Appeal to Anger/Outrage
- **Technique**: [Describe how the article provokes anger, e.g., "Framing an issue as a grave injustice," "Identifying a scapegoat."].
- **Example**: "[Quote a specific phrase or sentence.]"
- **Impact Score**: [1-10].

### Appeal to Hope/Salvation
- **Technique**: [Describe how the article offers an overly simplified or miraculous solution, e.g., "Presenting a leader as a savior," "Promising a utopian outcome."].
- **Example**: "[Quote a specific phrase or sentence.]"
- **Impact Score**: [1-10].

### Overall Manipulativeness Score
- **Score**: [Average of the impact scores.]
- **Reader Advisory**: [One-sentence advice, e.g., "The reader should be aware that the article's arguments are heavily reliant on provoking fear rather than presenting neutral evidence."].
</output>
""",
            "propaganda_detection": """
You are a propaganda detection AI. Analyze the provided text for classic and modern propaganda techniques. For each technique found, provide a clear example from the text.

**Article**: {article_text}

<output>
## Propaganda Technique Analysis

<technique>
  <name>Name-Calling</name>
  <present>[Yes/No]</present>
  <example>[If Yes, quote the example from the text, e.g., "Referring to opponents as 'traitors'."]</example>
</technique>

<technique>
  <name>Glittering Generalities</name>
  <present>[Yes/No]</present>
  <example>[If Yes, quote the example, e.g., "Using vague but positive terms like 'freedom' and 'strength' without specific details."]</example>
</technique>

<technique>
  <name>Card Stacking</name>
  <present>[Yes/No]</present>
  <example>[If Yes, describe the one-sided evidence presented, e.g., "Article lists five arguments for a policy but zero arguments against it."]</example>
</technique>

<technique>
  <name>Bandwagon</name>
  <present>[Yes/No]</present>
  <example>[If Yes, quote the example, e.g., "'A growing number of citizens agree that this is the only way forward'."]</example>
</technique>

<technique>
  <name>Whataboutism</name>
  <present>[Yes/No]</present>
  <example>[If Yes, describe the deflection, e.g., "When criticized about Topic A, the author deflects by bringing up an unrelated flaw in Topic B."]</example>
</technique>

### Summary
- **Primary Technique Used**: [Identify the most dominant propaganda technique.]
- **Propaganda Intensity**: [Low/Medium/High] - Based on the frequency and severity of the techniques used.
</output>
"""
        }

    def _get_evidence_evaluator_prompts(self) -> Dict[str, str]:
        """Evidence evaluation prompts, upgraded for extreme URL specificity and structured output."""
        return {
            "evidence_evaluation": """
You are 'Certus', an AI evidence evaluator. Your task is to provide specific, actionable, and directly relevant verification links for claims within the provided article.

**CRITICAL RULE**: You are forbidden from providing generic homepage URLs. Every URL must be a deep link to a specific article, report, or dataset that directly addresses the claim.

<article_data>
  <text>{article_text}</text>
  <claims>{extracted_claims}</claims>
</article_data>

<output>
## Evidence Verification Dossier

Provide the top 3 most relevant verification sources you can find.

<evidence_item>
  <claim>"[Quote the exact claim from the article being verified.]"</claim>
  <source_url>[The full, specific URL of the verification source.]</source_url>
  <source_type>[Primary Document, Peer-Reviewed Study, Statistical Database, Reputable News Report]</source_type>
  <conclusion>[State clearly whether this source CONFIRMS, CONTRADICTS, or PROVIDES CONTEXT for the claim.]</conclusion>
  <summary>[Provide a one-sentence summary of what the evidence shows. e.g., "The linked CDC report shows a 15% increase, contradicting the article's claim of 50%."]</summary>
</evidence_item>

<evidence_item>
  <claim>"[Quote the next claim.]"</claim>
  <source_url>[The full, specific URL of the verification source.]</source_url>
  <source_type>[Primary Document, Peer-Reviewed Study, Statistical Database, Reputable News Report]</source_type>
  <conclusion>[CONFIRMS / CONTRADICTS / PROVIDES CONTEXT]</conclusion>
  <summary>[One-sentence summary of the evidence.]</summary>
</evidence_item>

### Final Check
Review the `source_url` fields. Are they all deep links? If not, correct them before finalizing.
</output>
""",
            "source_quality": """
You are a source quality auditor. Evaluate the portfolio of sources cited in an article.

<input>
  <article_text>{article_text}</article_text>
  <source_list>{source_list}</source_list>
</input>

<output>
## Source Portfolio Quality Report

### Source Diversity & Balance
- **Analysis**: [Evaluate the range of sources. Are they all from one political viewpoint? Is there a mix of primary, secondary, and expert sources? Or does the article rely on a single type of source?]
- **Diversity Score**: [Score from 1-10, where 10 is a highly diverse and balanced portfolio.]

### Source Specificity & Verifiability
- **Analysis**: [Are the sources cited with specific links, titles, and dates? Or are they vague references like "studies show" or "experts say"? How many of the sources provide specific, article-level links vs. generic homepages?]
- **Verifiability Score**: [Score from 1-10, where 10 means all sources are specific and easily verifiable.]

### Overall Portfolio Assessment
- **Portfolio Quality Score**: [Average of the scores above, out of 10.]
- **Primary Weakness**: [Identify the single biggest flaw in the article's sourcing, e.g., "Lack of primary sources," "Reliance on biased commentators," "Vague, non-verifiable references."]
- **Recommendation**: [e.g., "The reader should independently verify the claims as the source portfolio is one-sided and lacks primary evidence."].
</output>
""",
            "logical_consistency": """
You are 'Logos', an AI specialist in logic and reasoning. Analyze the provided article for logical fallacies and structural consistency.

<input>
  <article_text>{article_text}</article_text>
  <key_claims>{key_claims}</key_claims>
</input>

<output>
## Logical Consistency Analysis

### Argument Structure
- **Main Thesis**: [State the central argument or conclusion of the article in one sentence.]
- **Primary Premises**: [List the 1-3 foundational assumptions or pieces of evidence upon which the thesis rests.]
- **Logical Flow**: [Assess whether the conclusion logically follows from the premises. Is the link strong, weak, or non-existent? e.g., "Weak. The conclusion makes a causal claim that is not sufficiently supported by the correlational evidence provided."].

### Logical Fallacy Detection
Identify up to three significant logical fallacies. If none are found, state so.

<fallacy>
  <name>[Name of the fallacy, e.g., "Straw Man"]</name>
  <example>"[Quote the exact text from the article that commits this fallacy.]"</example>
  <explanation>[Briefly explain how this example misrepresents the opposing argument.]</explanation>
</fallacy>

<fallacy>
  <name>[Name of the fallacy, e.g., "Hasty Generalization"]</name>
  <example>"[Quote the example.]"</example>
  <explanation>[Briefly explain how a broad conclusion is drawn from insufficient evidence.]</explanation>
</fallacy>

### Final Logic Assessment
- **Logic Quality Score**: [Score from 1-10, where 10 is a perfectly sound, well-reasoned argument.]
- **Critical Thinking Advisory**: [Provide one key question a reader should ask themselves while reading this article. e.g., "Does the evidence presented truly support the sweeping conclusion being drawn?"]
</output>
""",
            "evidence_gaps": """
You are an investigative editor. Your job is to find what's MISSING from an article by identifying evidence gaps.

<input>
  <article_text>{article_text}</article_text>
  <claims>{extracted_claims}</claims>
</input>

<output>
## Evidence Gap Report

### 1. Missing Data or Statistics
- **Identified Gap**: [What specific number, statistic, or data point is claimed without sourcing, or is needed to support a major claim? e.g., "The article claims rising crime rates but provides no statistical data from a law enforcement agency to support this."]
- **Question this Gap Raises**: [e.g., "Is the claim of 'rising crime' based on empirical data or just anecdotal evidence?"]

### 2. Missing Perspectives or Counterarguments
- **Identified Gap**: [What relevant expert opinion, stakeholder viewpoint, or common counterargument is not mentioned or addressed? e.g., "The article advocating for a new technology fails to interview any experts who have raised concerns about its potential safety risks."]
- **Question this Gap Raises**: [e.g., "Is the presentation of the technology intentionally one-sided to obscure potential downsides?"]

### 3. Missing Context or Background
- **Identified Gap**: [What crucial background information or context is omitted that could change a reader's interpretation of the events? e.g., "The article discusses a politician's controversial vote but omits the fact that it was part of a larger, bipartisan compromise bill."]
- **Question this Gap Raises**: [e.g., "Is the politician's action being presented out of context to make it appear more controversial than it was?"]

### Overall Evidence Completeness
- **Score**: [Score from 1-10, where 10 is comprehensive and addresses all key facets of the topic.]
- **Recommendation**: [Advise the reader on what type of information they should seek to get a fuller picture. e.g., "The reader should seek out official crime statistics and reports from non-partisan think tanks."]
</output>
"""
        }

    def get_prompt_template(self, agent_name: str, prompt_type: str) -> str:
        """Get specific prompt template"""
        agent_prompts = {
            "llm_explanation": self.llm_explanation_prompts,
            "credible_source": self.credible_source_prompts,
            "claim_extractor": self.claim_extractor_prompts,
            "context_analyzer": self.context_analyzer_prompts,
            "evidence_evaluator": self.evidence_evaluator_prompts
        }
        
        if agent_name not in agent_prompts:
            raise ValueError(f"Unknown agent: {agent_name}")
        
        if prompt_type not in agent_prompts[agent_name]:
            raise ValueError(f"Unknown prompt type '{prompt_type}' for agent '{agent_name}'")
            
        return agent_prompts[agent_name][prompt_type]

    def update_prompt_template(self, agent_name: str, prompt_type: str, new_template: str):
        """Update a specific prompt template"""
        agent_prompts = {
            "llm_explanation": self.llm_explanation_prompts,
            "credible_source": self.credible_source_prompts,
            "claim_extractor": self.claim_extractor_prompts,
            "context_analyzer": self.context_analyzer_prompts,
            "evidence_evaluator": self.evidence_evaluator_prompts
        }
        
        if agent_name not in agent_prompts:
            raise ValueError(f"Unknown agent: {agent_name}")
            
        agent_prompts[agent_name][prompt_type] = new_template

    def list_available_prompts(self) -> Dict[str, List[str]]:
        """List all available prompt templates by agent"""
        return {
            "llm_explanation": list(self.llm_explanation_prompts.keys()),
            "credible_source": list(self.credible_source_prompts.keys()),
            "claim_extractor": list(self.claim_extractor_prompts.keys()),
            "context_analyzer": list(self.context_analyzer_prompts.keys()),
            "evidence_evaluator": list(self.evidence_evaluator_prompts.keys())
        }

    def save_to_file(self, file_path: Optional[Path] = None):
        """Save all prompt templates to JSON file"""
        if file_path is None:
            from .settings import get_settings
            settings = get_settings()
            file_path = settings.project_root / "config" / "prompts.json"
            
        data = {
            "prompt_version": self.prompt_version,
            "llm_explanation_prompts": self.llm_explanation_prompts,
            "credible_source_prompts": self.credible_source_prompts,
            "claim_extractor_prompts": self.claim_extractor_prompts,
            "context_analyzer_prompts": self.context_analyzer_prompts,
            "evidence_evaluator_prompts": self.evidence_evaluator_prompts
        }
        
        with open(file_path, 'w') as f:
            json.dump(data, f, indent=2)

    @classmethod
    def load_from_file(cls, file_path: Path) -> 'PromptsConfig':
        """Load prompt templates from JSON file"""
        with open(file_path, 'r') as f:
            data = json.load(f)
        return cls(**data)

# Global prompts config instance
_prompts_config_instance: Optional[PromptsConfig] = None

def get_prompt_template(agent_name: str, prompt_type: str) -> str:
    """Get specific prompt template"""
    global _prompts_config_instance
    if _prompts_config_instance is None:
        _prompts_config_instance = PromptsConfig()
    return _prompts_config_instance.get_prompt_template(agent_name, prompt_type)

def update_prompt_template(agent_name: str, prompt_type: str, new_template: str):
    """Update a specific prompt template"""
    global _prompts_config_instance
    if _prompts_config_instance is None:
        _prompts_config_instance = PromptsConfig()
    _prompts_config_instance.update_prompt_template(agent_name, prompt_type, new_template)

def list_available_prompts() -> Dict[str, List[str]]:
    """List all available prompt templates by agent"""
    global _prompts_config_instance
    if _prompts_config_instance is None:
        _prompts_config_instance = PromptsConfig()
    return _prompts_config_instance.list_available_prompts()
